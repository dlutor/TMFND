# -*- coding: UTF-8 -*-
# import contextlib
import numpy as np
import random
import shutil
import os

import torch
# from pathlib import Path

import logging
import time
from datetime import timedelta

def check_dirs(dirs):
    if not os.path.exists(dirs):
        os.makedirs(dirs)
    return dirs


def set_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def save_checkpoint(state, is_best, checkpoint_path, filename="checkpoint.pt"):
    filename = os.path.join(checkpoint_path, filename)
    torch.save(state, filename)
    if is_best:
        shutil.copyfile(filename, os.path.join(checkpoint_path, "model_best.pt"))


def load_checkpoint(model, path):
    best_checkpoint = torch.load(path)
    model.load_state_dict(best_checkpoint["state_dict"])
    return best_checkpoint


class LogFormatter:
    def __init__(self):
        self.start_time = time.time()

    def format(self, record):
        elapsed_seconds = round(record.created - self.start_time)

        prefix = "%s - %s - %s" % (
            record.levelname,
            time.strftime("%x %X"),
            timedelta(seconds=elapsed_seconds),
        )
        message = record.getMessage()
        message = message.replace("\n", "\n" + " " * (len(prefix) + 3))
        return "%s - %s" % (prefix, message)


def create_logger(filepath, args):
    # create log formatter
    log_formatter = LogFormatter()

    # create file handler and set level to debug
    file_handler = logging.FileHandler(filepath, "a")
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(log_formatter)

    # create console handler and set level to info
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(log_formatter)

    # create logger and set level to debug
    logger = logging.getLogger()
    logger.handlers = []
    logger.setLevel(logging.INFO)
    logger.propagate = False
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    # reset logger elapsed time
    def reset_time():
        log_formatter.start_time = time.time()

    logger.reset_time = reset_time

    logger.info(
        "\n".join(
            "%s: %s" % (k, str(v))
            for k, v in sorted(dict(vars(args)).items(), key=lambda x: x[0])
        )
    )

    return logger

def collate(batch):
    elem = batch[0]
    elem_type = type(elem)
    if isinstance(elem, (list, tuple)):
        return elem_type((collate(samples) for samples in zip(*batch)))
    elif isinstance(elem, dict):
        return elem_type({key: collate([d[key] for d in batch]) for key in elem})
    elif len(elem.size()) == 1:
        return torch.hstack(batch)
    else:
        return torch.vstack(batch)

cpu_fn = lambda x:x.cpu()
cuda_fn = lambda x:x.cuda()
def copy_fn(elem, elem_fn=cpu_fn):
    elem_type = type(elem)
    if isinstance(elem, (list, tuple)):
        return elem_type((copy_fn(samples, elem_fn) for samples in elem))
    elif isinstance(elem, dict):
        return elem_type({key: copy_fn(elem[key], elem_fn) for key in elem})
    else:
        return elem_fn(elem)


class AverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


def count_parameters(model):
    params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return params / 1000000






